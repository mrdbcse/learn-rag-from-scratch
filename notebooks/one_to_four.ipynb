{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd064ae0",
   "metadata": {},
   "source": [
    "#### Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cae27f",
   "metadata": {},
   "source": [
    "###### API Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c2f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda22a4d",
   "metadata": {},
   "source": [
    "#### Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b468d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> str:\n",
    "    enc = tiktoken.get_encoding(encoding_name=encoding_name)\n",
    "    return len(enc.encode(text=text))\n",
    "\n",
    "\n",
    "def format_documents(docs: list[Document]) -> str:\n",
    "    combined = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9603d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 8952\n",
      "The article surveys how to build LLM-centered autonomous agents, positioning the LLM as the “brain” augmented by planning (task decomposition and reflection), memory (short-term context plus long-term vector-store retrieval/MIPS), and tool use (API calling and external models). It reviews techniques such as Chain/Tree of Thought, LLM+P, ReAct, Reflexion, Chain of Hindsight, Algorithm Distillation; retrieval methods (LSH, HNSW, FAISS, ScaNN); and tool frameworks/benchmarks like MRKL, Toolformer, HuggingGPT, and API-Bank, with case studies including ChemCrow, scientific discovery agents, and Generative Agents. The piece highlights PoCs (AutoGPT, GPT-Engineer) and key challenges: limited context windows, hard long-horizon planning/adaptation, and unreliable natural-language interfaces.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# INDEXING\n",
    "\n",
    "# Load documents\n",
    "docs = PyPDFLoader(\"../documents/NIPS-2017-attention-is-all-you-need-Paper.pdf\").load()\n",
    "\n",
    "# Split\n",
    "split_docs = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=200\n",
    ").split_documents(documents=docs)\n",
    "\n",
    "combined = format_documents(docs=split_docs)\n",
    "tokens = count_tokens(text=combined)\n",
    "\n",
    "print(f\"Total Tokens: {tokens}\")\n",
    "\n",
    "\n",
    "# Embed\n",
    "vector_store = Chroma.from_documents(documents=split_docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"Summarize this document\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c9d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
